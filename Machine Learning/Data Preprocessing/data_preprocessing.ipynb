{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My independent variables are: [['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 nan]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' nan 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n",
      "My depdentent variabel is: ['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "df = pd.read_csv(r'C:\\Users\\Stelios_Ntanavaras\\Documents\\Python Scripts\\Machine-Learning-A-Z-AI-Python-R-ChatGPT-Prize-2024-\\Machine Learning\\Data.csv')\n",
    "df.head()\n",
    "\n",
    "# Split my data to X (independent variables) and y (dependent variable)\n",
    "# .iloc : locate indexes, take the indexes of the columns we want to extract from the dataset\n",
    "X = df.iloc[:, :-1].values # -1 in Python is the index of the last column (exluded)\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "print('My independent variables are:', X)\n",
    "print('My depdentent variabel is:', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "# Taking care of Missing Data (avg method)\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Replace the missing value by the avg of values of the column the missing value belongs to\n",
    "imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
    "imputer.fit(X[:, 1:3]) # The '.fit' method looks for only numerical missing values, that is why we exclude the first column (categorical)\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3]) # The '.transform' method does the transformation from missing data to the avg of all values\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [0.0 1.0 0.0 30.0 54000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 35.0 58000.0]\n",
      " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "# Encoding Categorical Data\n",
    "\n",
    "# We could just tag each categorical data into a number, like Germany = 0, France = 1, and Spain = 2.\n",
    "# But in this way our model will understand that there is a numerical order between these three countries.\n",
    "# But we want to avoid the model to understand this correlation because it is not true, we want the numbers just to be labels.\n",
    "# So, we are going to do OneHotEncoding (where there is no numerical order).\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ct = ColumnTransformer(transformers = [('encoder', OneHotEncoder(), [0])], remainder = 'passthrough') # 'remainder' arg is to keep the columns that the transformation is not applied (Age and Salary cols)\n",
    "X = np.array(ct.fit_transform(X))\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Encoding the Dependent Variable y\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y) # doesn't need to be a numpy array\n",
    "\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply Feature Scaling !!! AFTER !!! splitting the dataset into training and testing set. That's because the test set suppose to be a brand new set where we are going to evaluate our model. If we do feature scaling on test set before the split we will have a test set leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into Training and Testing set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we need to apply Standardization on feature columns with dummy variables? !!! NO !!! i.e. if we apply this on dummy vectors like OneHot vectors, we will lose the interpretation of which country belongs to which vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n",
      " [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n",
      " [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n",
      " [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n",
      " [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n",
      " [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n",
      " [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n",
      " [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n",
      "[[0.0 1.0 0.0 -1.0 -1.0]\n",
      " [1.0 0.0 0.0 1.0 1.0]]\n"
     ]
    }
   ],
   "source": [
    "# Feature Scaling (purpose: prevent some features dominate other features)\n",
    "# Feature Scaling is always applying to COLUMNS, not ROWS.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# First Technique: STANDARDIZATION (almost all the values inside the columns will be mapped in a range of [-3,3])\n",
    "# stand = x - mean(x) / std(x)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train[:, 3:] = sc.fit_transform(X_train[:, 3:]) # '.fit' will find the mean and std of all values, and '.transform' will apply the formula\n",
    "X_test[:, 3:] = sc.fit_transform(X_test[:, 3:])\n",
    "\n",
    "print(X_train)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Technique: NORMALIZATION (almost all the values inside the columns will be mapped into a range of [0,1])\n",
    "# norm = x - min(x) / max(x) - min(x)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
